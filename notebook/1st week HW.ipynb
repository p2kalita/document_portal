{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7c4d36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000024009859EA0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000240098596C0>, root_client=<openai.OpenAI object at 0x000002400985A110>, root_async_client=<openai.AsyncOpenAI object at 0x0000024009859F60>, model_name='gpt-4o', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.1\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01871f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000024009A0B910>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x000002400C644F70>, model='models/embedding-001', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "252f1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,PyPDFLoader\n",
    "\n",
    "pdf_directory = r\"C:\\Users\\ParthaPratimKalita\\document_portal\\notebook\\data_hw\"\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\n",
    "    path = r\"C:\\Users\\ParthaPratimKalita\\document_portal\\notebook\\data_hw\",\n",
    "    glob = \"**/[!.]*.pdf\",\n",
    "    silent_errors = False,\n",
    "    load_hidden = False,\n",
    "    recursive = False,\n",
    "    extract_images = False,\n",
    "    password = None,\n",
    "    mode = \"page\",\n",
    "    # images_to_text = None,\n",
    "    headers = None,\n",
    "    extraction_mode = \"plain\",\n",
    "    # extraction_kwargs = None,\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab5b3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCOUNTING                               \n",
      "BRENDA VIEUX                                              \n",
      "{'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'keywords': '', 'source': 'C:\\\\Users\\\\ParthaPratimKalita\\\\document_portal\\\\notebook\\\\data_hw\\\\burgbcwhu_0 burgbhkqt_3122.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}\n"
     ]
    }
   ],
   "source": [
    "docs = await loader.aload()\n",
    "print(docs[6].page_content[:100])\n",
    "print(docs[6].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49e11733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "181bf568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitters = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150\n",
    ")\n",
    "\n",
    "chunks = text_splitters.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08960bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2400984fc10>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7bc58621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002400984FC10>, search_kwargs={'k': 10})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce452cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. You will be given a question and a set of documents.\\nYour task is to answer the question based on the documents provided.\\n\\nContext : {context}\\n\\nQuestion : {question}\\n\\n')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template= \"\"\"\n",
    "You are a helpful assistant. You will be given a question and a set of documents.\n",
    "Your task is to answer the question based on the documents provided.\n",
    "\n",
    "Context : {context}\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb801a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88eecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "67c5c607",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\" : retriever | format_docs, \"question\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "332b1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Llama 2 fine-tuning benchmark experiments involve evaluating the performance of various Llama 2 models across a suite of popular academic and standard benchmarks. The models are compared based on their performance on these benchmarks, which are grouped into different categories. The results are summarized in tables that show the performance of different Llama 2 models, such as 7B, 13B, 34B, and 70B, across these benchmarks.\\n\\nFor instance, in the context provided, the Llama 2 models are evaluated on grouped academic benchmarks, with scores indicating their performance. The 70B model generally shows the highest performance across these benchmarks, followed by the 34B, 13B, and 7B models. The performance is measured in various metrics, such as pass rates for specific tasks like Human-Eval and MBPP.\\n\\nAdditionally, the fine-tuning experiments also compare Llama 2 models with other models like MPT and Falcon, showing how Llama 2 models perform relative to these other models on tasks like pass@1 and pass@100 for Human-Eval and pass@1 and pass@80 for MBPP.\\n\\nOverall, the experiments aim to assess the effectiveness of Llama 2 models in handling a range of tasks and benchmarks, providing insights into their capabilities and areas where they excel compared to other models.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"Tell me about the llama2 finetning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab6bd49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The scaling trends for the reward models in Llama 2 models involve fine-tuning different model sizes on an increasing amount of reward model data collected weekly. As more preference data is collected, the reward models improve, allowing for progressively better versions of Llama 2-Chat. The expected result is that larger models perform better with more data. Additionally, it is important to gather new preference data using the latest Llama 2-Chat iterations before a new tuning iteration to keep the reward model on-distribution and maintain accurate rewards.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"what is the Scaling trends for the reward models in Llama 2 models?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
